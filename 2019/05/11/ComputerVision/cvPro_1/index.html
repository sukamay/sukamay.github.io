<h1 id="Assignment-2"><a href="#Assignment-2" class="headerlink" title="Assignment 2"></a>Assignment 2</h1><p>ing RANSAC in <code>hw2_1_1652820_ransac.m</code> (running in Matlab 2016b).</p>
<p>​    The result of fitting model from sample points is as follows.</p>
<p>​    Just change variable x and y if you want to test other sample points in 2D platform.</p>
<p><img src="/images/ransac.png" alt="ransac"></p>
<h2 id="Problem-2"><a href="#Problem-2" class="headerlink" title="Problem 2:"></a>Problem 2:</h2><p>​    Test demo is available in <code>hw2_1_1652820_AdaBoost.m</code> (runnning in Matlab 2016b), just run the script then you will see the result of my training model and test data.</p>
<p>​    If you want to change parameters of training model, turn to <code>trainClassifier.m</code>,  parameters are no need to change except parameter T, data or show. </p>
<p>​    <strong>T</strong> is times of iteration which changes according to concrete applications. </p>
<p>​    <strong>Data</strong> is training data. </p>
<p>​    <strong>Show</strong> is switch which you chould decide whether the classify process will be shown graphically or not.</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% result of given training data:</span></span><br><span class="line"><span class="comment">% I use T = 3 in my implementation</span></span><br><span class="line"><span class="comment">% alpha = [0.4236 0.6496 0.9229]</span></span><br><span class="line"><span class="comment">% weak classifier = [306 1;</span></span><br><span class="line"><span class="comment">%											86 3;</span></span><br><span class="line"><span class="comment">%										 135 1]</span></span><br><span class="line"><span class="comment">% precision: 100%</span></span><br></pre></td></tr></table></figure>
<p><img src="/images/adaboost.png" alt="adaboost"></p>
<h2 id="Problem-3"><a href="#Problem-3" class="headerlink" title="Problem 3:"></a>Problem 3:</h2><h3 id="Method-1"><a href="#Method-1" class="headerlink" title="Method 1:"></a>Method 1:</h3><p>​    Thus, $\alpha_1$ should be the Eigen-vector of  $C$  corresponding to the largest eigen-value of $C$. </p>
<p>​    Now we try to find another orientation $\alpha_2$ , orthogonal to  $\alpha_1$ , and along which the data can have the second largest variation.</p>
<p>​    Based on Lagrange multipiler method, we need to,<br>$$<br>arg \space max_{\alpha}(\alpha^TC\alpha - \lambda_1 \alpha_1^T \alpha - \lambda_2 (\alpha^T\alpha - 1))<br>$$<br>​    The first partial derivative of function $F =  \alpha^TC\alpha - \lambda_1 \alpha_1^T \alpha - \lambda_2 (\alpha^T\alpha - 1)$ with respect to each its independent variable is :<br>$$<br>\begin{align}<br>\frac{\partial F}{\partial \alpha} &amp; = 2 C \alpha - \lambda_1 \alpha_1 - 2\lambda_2 \alpha = 0 \<br>\frac{\partial F}{\partial \lambda_1} &amp;= \alpha_1^T \alpha = 0 \ \frac{\partial F}{\partial \lambda_2} &amp;= \alpha^T \alpha - 1 = 0<br>\end{align}<br>$$<br>     Formula (2) could be transformed to:<br>$$<br>\begin{align} \nonumber 2C\alpha - \lambda_1 \alpha_1 - 2 \lambda_2 \alpha &amp;=  0 \<br>2C\alpha \alpha^T - \lambda_1 \alpha_1 \alpha^T - 2 \lambda_2 \alpha \alpha^T &amp;=  0<br>\end{align}<br>$$<br>​    Bring into Formula (3), we get:<br>$$<br>\begin{align}<br>\nonumber 2C \alpha \alpha^T - 2 \lambda_2 \alpha \alpha^T &amp;=  0 \<br>2C \alpha \alpha^T \alpha - 2 \lambda_2 \alpha \alpha^T \alpha &amp;=  0<br>\end{align}<br>$$<br>​    Bring into Formula(4), we get:<br>$$<br>\begin{align}<br>\nonumber 2C \alpha - 2\lambda_2 \alpha &amp;= 0 \<br>C\alpha &amp;= \lambda_2 \alpha<br>\end{align}<br>$$<br>​    From Formula (7) we could know $\lambda_2$ is $C’s$ eigen-vector.</p>
<p>​    Since,<br>$$<br>max(var(\alpha^TX)) = max(\alpha^TC\alpha) \ = max(\alpha^T \lambda_2 \alpha) = max(\lambda_2)<br>$$<br>​    Suppose $\alpha_2$ to be  the eigen-vector of $C$ corresponding to $\lambda_2$ . We want the eigen-value of $C$ corresponding to $\alpha_2$ to be largest under the constraints of $\alpha_2$ is orthogonal to $\alpha_1$ .<br>$$<br>\begin{align} \nonumber C &amp;= \frac{1}{n-1} \sum_{i=1}^n (x_i - \mu) (x_i - \mu )^T \<br>\nonumber  C^T &amp;= \frac{1}{n-1} \sum_{i=1}^n ((x_i - \mu) (x_i - \mu )^T )^T \<br>\nonumber  C^T &amp;= \frac{1}{n-1} \sum_{i=1}^n (x_i - \mu) (x_i - \mu )^T \<br>&amp; = C<br>\end{align}<br>$$<br>​    So the covariance matrix $C$ is a symmetric matrix. On linear algebra, the real symmetric matrix has a very good property:</p>
<blockquote>
<p>The eigen-vectors corresponding to different eigen-values of the real symmetric matrix must be orthogonal.</p>
</blockquote>
<p>​    So the second largest eigen-value of $C$ is what we want! In such condition, we get that $\alpha_2$ is the eigen-vector associated to the second largest eigen-value $\lambda_2$ of $C$ and the second largest variance is $\lambda_2$.</p>
<h2 id="Problem-4"><a href="#Problem-4" class="headerlink" title="Problem 4:"></a>Problem 4:</h2><p>$$<br>J(\theta) = -\sum_{i=1}^{m}y_i log(h_{\theta}(x_i)) + (1-y_i)log(1-h_{\theta}(x_i))<br>$$</p>
<p>$$<br>where \space h_{\theta}(x_i) = \frac{1}{1+exp(-\theta^Tx)}<br>$$</p>
<p>$$<br>\begin{align} \nonumber \frac{\partial J(\theta)}{\partial \theta_j} &amp; = \frac{-\sum_{i=1}^{m}y_i log(h_{\theta}(x_i)) + (1-y_i)log(1-h_{\theta}(x_i))}{\partial \theta_j} \<br>\nonumber &amp; = -\sum_{i=1}^{m} \frac{y_i log(h_{\theta}(x_i)) + (1-y_i)log(1-h_{\theta}(x_i))}{\partial \theta_j} \<br> &amp; = -\sum_{i=1}^{m}y_i \frac{1}{h_{\theta}(x_i)} \frac{\partial h_{\theta}(x_i)}{\partial \theta_j} + (1-y_i) \frac{1}{1-h_{\theta}(x_i)} \frac{\partial(1-h_{\theta}(x_i))}{\partial \theta_j}\<br>\end{align}<br>$$</p>
<p>Now we first caculate the result of $\frac{\partial h_{\theta}(x_i)}{\partial \theta_j}$ :<br>$$<br>\begin{align} \nonumber \frac{\partial h_{\theta}(x_i)}{\partial \theta_j} &amp;=  \frac{\partial(\frac{1}{1+exp(-\theta^Tx_i)})}{\partial \theta_j} \ \nonumber &amp; \xlongequal{z = \theta^Tx_i}   \frac{\partial h(z) }{\partial z} \frac{\partial z}{\partial \theta_j}  \  \nonumber &amp; =  h(z)(1-h(z)) \frac{\partial (\theta^Tx_i)}{\partial \theta_j}\ \nonumber &amp; =  h(z)(1-h(z)) x_{ij} \ &amp;= h_{\theta}(x_i)(1-h_{\theta}(x_i))x_{ij} \end{align}<br>$$<br>According to Formula (4), Formula (3) could be simplized as follows:<br>$$<br>\begin{align}<br>\nonumber \frac{\partial J(\theta)}{\partial \theta_j} &amp; = -\sum_{i=1}^{m}y_i \frac{1}{h_{\theta}(x_i)} \frac{\partial h_{\theta}(x_i)}{\partial \theta_j} + (1-y_i) \frac{1}{1-h_{\theta}(x_i)} \frac{\partial(1-h_{\theta}(x_i))}{\partial \theta_j}\<br>\nonumber &amp;= -\sum_{i=1}^m y_i \frac{1}{h_{\theta}(x_i)}  h_{\theta}(x_i)(1-h_{\theta}(x_i))x_{ij}  \<br>\nonumber &amp; \space \space \space \space + (1-y_i) \frac{1}{1-h_{\theta}(x_i)} h_{\theta}(x_i)(1-h_{\theta}(x_i))x_{ij}  \<br>\nonumber &amp; = -\sum_{i=1}^m y_i (1-h_{\theta}(x_i))x_{ij} + (1-y_i)h_{\theta}(x_i)x_{ij} \<br>&amp;=\sum_{i=1}^m(h_{\theta}(x_i)-y_i)x_{ij}<br>\end{align}<br>$$<br>So we get $\bigtriangledown_{\theta}J(\theta)$ :<br>$$<br>\bigtriangledown_{\theta}J(\theta) = \begin{bmatrix} \frac{\partial J(\theta)}{\partial \theta_1} \ \frac{\partial J(\theta)}{\partial \theta_2} \ . \ . \ . \  \frac{\partial J(\theta)}{\partial \theta_{d+1}} \end{bmatrix} =  \begin{bmatrix} \sum_{i=1}^m(h_{\theta}(x_i)-y_i)x_{i1} \ \sum_{i=1}^m(h_{\theta}(x_i)-y_i)x_{i2}<br>\ . \ . \ .<br>\ \sum_{i=1}^m(h_{\theta}(x_i)-y_i)x_{i(d+1)}<br>\end{bmatrix}<br>= \sum_{i=1}^m x_i(h_{\theta}(x_i) - y_i)<br>$$</p>
<h2 id="Problem-5"><a href="#Problem-5" class="headerlink" title="Problem 5:"></a>Problem 5:</h2>