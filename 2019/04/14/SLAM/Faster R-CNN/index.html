<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta name="description" content="Read time: 21 minutes Reading notes by may 2019.4.13 Confused:  ==ReLU== ==max pooling== ==softmax== ==Fully-Connected layers== ==convolutional layers== ==Non-maximum Suppression==: specific implement">
<meta property="og:type" content="article">
<meta property="og:title" content="May">
<meta property="og:url" content="https://sukamay.github.io/2019/04/14/SLAM/Faster R-CNN/index.html">
<meta property="og:site_name" content="May">
<meta property="og:description" content="Read time: 21 minutes Reading notes by may 2019.4.13 Confused:  ==ReLU== ==max pooling== ==softmax== ==Fully-Connected layers== ==convolutional layers== ==Non-maximum Suppression==: specific implement">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://tryolabs.com/images/blog/post-images/2018-01-18-faster-rcnn/fasterrcnn-architecture.b9035cba.png">
<meta property="og:image" content="https://tryolabs.com/images/blog/post-images/2018-01-18-faster-rcnn/vgg.b6e48b99.png">
<meta property="og:image" content="https://tryolabs.com/images/blog/post-images/2018-01-18-faster-rcnn/image-to-feature-map.89f5aecb.png">
<meta property="og:image" content="https://tryolabs.com/images/blog/post-images/2018-01-18-faster-rcnn/anchors-centers.141181d6.png">
<meta property="og:image" content="https://tryolabs.com/images/blog/post-images/2018-01-18-faster-rcnn/anchors-progress.119e1e92.png">
<meta property="og:image" content="https://tryolabs.com/images/blog/post-images/2018-01-18-faster-rcnn/rpn-architecture.99b6c089.png">
<meta property="og:image" content="https://tryolabs.com/images/blog/post-images/2018-01-18-faster-rcnn/rpn-conv-layers.63c5bf86.png">
<meta property="og:image" content="https://tryolabs.com/images/blog/post-images/2018-01-18-faster-rcnn/roi-architecture.7eaae6c2.png">
<meta property="og:image" content="https://tryolabs.com/images/blog/post-images/2018-01-18-faster-rcnn/rcnn-architecture.6732b9bd.png">
<meta property="og:updated_time" content="2019-05-10T16:24:36.995Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="May">
<meta name="twitter:description" content="Read time: 21 minutes Reading notes by may 2019.4.13 Confused:  ==ReLU== ==max pooling== ==softmax== ==Fully-Connected layers== ==convolutional layers== ==Non-maximum Suppression==: specific implement">
<meta name="twitter:image" content="https://tryolabs.com/images/blog/post-images/2018-01-18-faster-rcnn/fasterrcnn-architecture.b9035cba.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://sukamay.github.io/2019/04/14/SLAM/Faster R-CNN/">





  <title> | May</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">May</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://sukamay.github.io/2019/04/14/SLAM/Faster R-CNN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="May">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/profile.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="May">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline"></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-14T12:45:44+08:00">
                2019-04-14
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/04/14/SLAM/Faster R-CNN/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/04/14/SLAM/Faster R-CNN/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          
            <span class="post-meta-divider">|</span>
            <span id="busuanzi_value_page_pv"></span>次阅读
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>Read time: 21 minutes</p>
<p>Reading notes by may</p>
<p>2019.4.13</p>
<p>Confused:</p>
<ol>
<li>==ReLU==</li>
<li>==max pooling==</li>
<li>==softmax==</li>
<li>==Fully-Connected layers==</li>
<li>==convolutional layers==</li>
<li>==Non-maximum Suppression==: specific implement?</li>
</ol>
<p>Warming Reading Material :</p>
<h1 id="Faster-R-CNN-Down-the-rabbit-hole-of-modern-object-detection"><a href="#Faster-R-CNN-Down-the-rabbit-hole-of-modern-object-detection" class="headerlink" title="Faster R-CNN: Down the rabbit hole of modern object detection"></a>Faster R-CNN: Down the rabbit hole of modern object detection</h1><p>Author : <a href="https://tryolabs.com/blog/authors/javier-rey/" target="_blank" rel="noopener">Javier</a> Thu, Jan 18, 2018 in <a href="https://tryolabs.com/blog/categories/machine-learning/" target="_blank" rel="noopener">MACHINE LEARNING</a></p>
<ul>
<li><a href="https://tryolabs.com/blog/tags/deep-learning/" target="_blank" rel="noopener">DEEP LEARNING</a></li>
<li><a href="https://tryolabs.com/blog/tags/object-detection/" target="_blank" rel="noopener">OBJECT DETECTION</a></li>
<li><a href="https://tryolabs.com/blog/tags/computer-vision/" target="_blank" rel="noopener">COMPUTER VISION</a></li>
<li><a href="https://tryolabs.com/blog/tags/luminoth/" target="_blank" rel="noopener">LUMINOTH</a></li>
</ul>
<p>Previously, we <a href="https://tryolabs.com/blog/2017/08/30/object-detection-an-overview-in-the-age-of-deep-learning/" target="_blank" rel="noopener">talked about object detection</a>, what it is and how it has been recently tackled using deep learning. If you haven’t read our previous blog post, we suggest you take a look at it before continuing.</p>
<p>Last year, we decided to get into Faster R-CNN, reading the original paper, and all the referenced papers (and so on and on) until we got a clear understanding of how it works and how to implement it.</p>
<p>We ended up implementing Faster R-CNN in <a href="http://github.com/tryolabs/luminoth" target="_blank" rel="noopener">Luminoth</a>, a computer vision toolkit based on TensorFlow which makes it easy to train, monitor and use these types of models. So far, Luminoth has raised an incredible amount of interest and we even talked about it at both <a href="https://www.youtube.com/watch?v=CAYn6A1zsrw" target="_blank" rel="noopener">ODSC Europe</a> and <a href="https://www.youtube.com/watch?v=SH8DCm_lW1Q" target="_blank" rel="noopener">ODSC West</a>.</p>
<p>Based on all the work developing Luminoth and based on the presentations we did, we thought it would be a good idea to have a blog post with all the details and links we gathered in our research as a future reference for anyone is interested in the topic.</p>
<h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><p><a href="https://arxiv.org/abs/1506.01497" target="_blank" rel="noopener">Faster R-CNN</a> was originally published in NIPS 2015. After publication, it went through a couple of revisions which we’ll later discuss. As we mentioned in our previous blog post, Faster R-CNN is the third iteration of the R-CNN papers — which had Ross Girshick as author &amp; co-author.</p>
<p>Everything started with “<a href="https://arxiv.org/abs/1311.2524" target="_blank" rel="noopener">Rich feature hierarchies for accurate object detection and semantic segmentation</a>” (R-CNN) in 2014, which used an algorithm called <a href="https://koen.me/research/selectivesearch/" target="_blank" rel="noopener">Selective Search</a> to propose possible regions of interest and a standard Convolutional Neural Network (CNN) to classify and adjust them. It quickly evolved into <a href="https://arxiv.org/abs/1504.08083" target="_blank" rel="noopener">Fast R-CNN</a>, published in early 2015, where a technique called Region of Interest Pooling allowed for sharing expensive computations and made the model much faster. Finally came <a href="https://arxiv.org/abs/1506.01497" target="_blank" rel="noopener">Faster R-CNN</a>, where the first fully differentiable model was proposed.</p>
<h1 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h1><p>The architecture of Faster R-CNN is complex because it has several moving parts. We’ll start with a high level overview, and then go over the details for each of the components.</p>
<p>It all starts with an image, from which we want to obtain:</p>
<ul>
<li>a list of bounding boxes.</li>
<li>a label assigned to each bounding box.</li>
<li>a probability for each label and bounding box.</li>
</ul>
<p><img src="https://tryolabs.com/images/blog/post-images/2018-01-18-faster-rcnn/fasterrcnn-architecture.b9035cba.png" alt="img"><div align="center">Complete Faster R-CNN architecture</div></p>
<p>The input images are represented as $Height×Width×Depth​$ tensors (multidimensional arrays), which are passed through a pre-trained CNN up until an intermediate layer, ending up with a convolutional feature map. We use this as a feature extractor for the next part.</p>
<p>This technique is very commonly used in the context of ==Transfer Learning==, especially for training a classifier on a small dataset using the weights of a network trained on a bigger dataset. We’ll take a deeper look at this in the following sections.</p>
<p>Next, we have what is called a Region Proposal Network (RPN, for short). Using the features that the CNN computed, it is used to find up to a <em>predefined number</em> of regions (bounding boxes), which may contain objects.</p>
<p>Probably the hardest issue with using Deep Learning (DL) for object detection is generating a variable-length list of bounding boxes. When modeling deep neural networks, the last block is usually a fixed sized tensor output (except when using Recurrent Neural Networks, but that is for another post). For example, in image classification, the output is a (N,)(<em>N</em>,) shaped tensor, with N<em>N</em> being the number of classes, where each scalar in location i<em>i</em> contains the probability of that image being $label_i​$.</p>
<p>The variable-length problem is solved in the RPN by using anchors: fixed sized reference bounding boxes which are placed uniformly throughout the original image. Instead of having to detect where objects are, we model the problem into two parts. For every anchor, we ask:</p>
<ul>
<li>Does this anchor contain a relevant object?</li>
<li>How would we adjust this anchor to better fit the relevant object?</li>
</ul>
<p>This is probably getting confusing, but fear not, we’ll dive into this below.</p>
<p>After having a list of possible relevant objects and their locations in the original image, it becomes a more straightforward problem to solve. Using the features extracted by the CNN and the bounding boxes with relevant objects, we apply Region of Interest (RoI) Pooling and extract those features which would correspond to the relevant objects into a new tensor.</p>
<p>Finally, comes the R-CNN module, which uses that information to:</p>
<ul>
<li>Classify the content in the bounding box (or discard it, using “background” as a label).</li>
<li>Adjust the bounding box coordinates (so it better fits the object).</li>
</ul>
<p>Obviously, some major bits of information are missing, but that’s basically the general idea of how Faster R-CNN works. Next, we’ll go over the details on both the architecture and loss/training for each of the components.</p>
<h1 id="Base-network"><a href="#Base-network" class="headerlink" title="Base network"></a>Base network</h1><p>As we mentioned earlier, the first step is using a CNN pretrained for the task of classification (e.g. using <a href="http://www.image-net.org/" target="_blank" rel="noopener">ImageNet</a>) and using the output of an intermediate layer. This may sound really simple for people with a deep learning background, but it’s important to understand how and why it works, as well as visualize what the intermediate layer output looks like.</p>
<p>There is no real consensus on which network architecture is best. The original Faster R-CNN used <a href="https://arxiv.org/abs/1311.2901" target="_blank" rel="noopener">ZF</a> and <a href="https://arxiv.org/abs/1409.1556" target="_blank" rel="noopener">VGG</a> pretrained on ImageNet but since then there have been lots of different networks with a varying number of weights. For example, <a href="https://arxiv.org/abs/1704.04861" target="_blank" rel="noopener">MobileNet</a>, a smaller and efficient network architecture optimized for speed, has approximately 3.3M parameters, while ResNet-152 (yes, 152 layers), once the state of the art in the ImageNet classification competition, has around 60M. Most recently, new architectures like <a href="https://arxiv.org/abs/1608.06993" target="_blank" rel="noopener">DenseNet</a> are both improving results while lowering the number of parameters.</p>
<h2 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a>VGG</h2><p>Before we talk about which is better or worse, let’s try to understand how it all works using the standard VGG-16 as an example.</p>
<p>==ReLU,max pooling, softmax==</p>
<p><img src="https://tryolabs.com/images/blog/post-images/2018-01-18-faster-rcnn/vgg.b6e48b99.png" alt="img">VGG architecture</p>
<p>VGG, whose name comes from the team which used it in the ImageNet ILSVRC 2014 competition, was published in the paper “Very Deep Convolutional Networks for Large-Scale Image Recognition” by <a href="http://www.robots.ox.ac.uk/~karen/" target="_blank" rel="noopener">Karen Simonyan</a> and <a href="http://www.robots.ox.ac.uk/~az/" target="_blank" rel="noopener">Andrew Zisserman</a>. By today’s standards it would not be considered very deep, but at the time it more than doubled the number of layers commonly used and kickstarted the “deeper → more capacity → better” wave (when training is possible).</p>
<p>When using VGG for classification, the input is a 224×224×3224×224×3 tensor (that means a 224x224 pixel RGB image). This has to remain fixed for classification because the final block of the network uses fully-connected (FC) layers (instead of convolutional), which require a fixed length input. This is usually done by flattening the output of the last convolutional layer, getting a rank 1 tensor, before using the FC layers.</p>
<p>Since we are going to use the output of an intermediate convolutional layer, the size of the input is not our problem. At least, it is not the problem of this module since only convolutional layers are used. Let’s get a bit more into low-level details and define which convolutional layer we are going to use. The paper does not specify which layer to use; but in the official implementation you can see they use the output of <code>conv5/conv5_1</code> layer.</p>
<p>Each convolutional layer creates abstractions based on the previous information. The first layers usually learn edges, the second finds patterns in edges in order to activate for more complex shapes and so forth. Eventually we end up with a convolutional feature map which has spatial dimensions much smaller than the original image, but greater depth. ==The width and height of the feature map decrease because of the pooling applied between convolutional layers and the depth increases based on the number of filters the convolutional layer learns.==</p>
<p><img src="https://tryolabs.com/images/blog/post-images/2018-01-18-faster-rcnn/image-to-feature-map.89f5aecb.png" alt="img">Image to convolutional feature map</p>
<p>In its depth, the convolutional feature map has encoded all the information for the image while maintaining the location of the “things” it has encoded relative to the original image. For example, if there was a red square on the top left of the image and the convolutional layers activate for it, then the information for that red square would still be on the top left of the convolutional feature map.</p>
<h2 id="VGG-vs-ResNet"><a href="#VGG-vs-ResNet" class="headerlink" title="VGG vs ResNet"></a>VGG vs ResNet</h2><p>Nowadays, ResNet architectures have mostly replaced VGG as a base network for extracting features. Three of the co-authors of Faster R-CNN (Kaiming He, Shaoqing Ren and Jian Sun) were also co-authors of “<a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="noopener">Deep Residual Learning for Image Recognition</a>”, the original paper describing ResNets.</p>
<p>The obvious advantage of ResNet over VGG is that it is bigger, hence it has more capacity to actually learn what is needed. This is true for the classification task and should be equally true in the case of object detection.</p>
<p>Also, ResNet makes it easy to train deep models with the use of <em>residual connections</em>and <em>batch normalization</em>, which was not invented when VGG was first released.</p>
<h1 id="Anchors"><a href="#Anchors" class="headerlink" title="Anchors"></a>Anchors</h1><p>Now that we are working with a processed image, we need to find proposals, ie. regions of interest for classification. We previously mentioned that anchors are a way to solve the variable length problem, but we skipped most of the explanation.</p>
<p>Our objective is to find bounding boxes in the image. These have rectangular shape and can come in different sizes and aspect ratios. Imagine we were trying to solve the problem knowing beforehand that there are two objects on the image. The first idea that comes to mind is to train a network that returns 8 values: two xmin,ymin,xmax,ymax<em>x<strong>m</strong>i**n</em>,<em>y<strong>m</strong>i**n</em>,<em>x<strong>m</strong>a**x</em>,<em>y<strong>m</strong>a**x</em> tuples defining a bounding box for each object. This approach has some fundamental problems. For example, images may have different sizes and aspect ratios, having a good model trained to predict raw coordinates can turn out to be very complicated (if not impossible). Another problem is invalid predictions: when predicting xmin<em>x<strong>m</strong>i**n</em> and xmax<em>x<strong>m</strong>a**x</em> we have to somehow enforce that xmin&lt;xmax<em>x<strong>m</strong>i**n</em>&lt;<em>x<strong>m</strong>a**x</em>.</p>
<p>It turns out that there is a simpler approach to predicting bounding boxes by learning to predict offsets from reference boxes. We take a reference box xcenter,ycenter,width,height<em>x<strong>c</strong>e<strong>n</strong>t<strong>e</strong>r</em>,<em>y<strong>c</strong>e<strong>n</strong>t<strong>e</strong>r</em>,<em>w<strong>i</strong>d<strong>t</strong>h</em>,<em>h<strong>e</strong>i<strong>g</strong>h**t</em> and learn to predict Δxcenter,Δycenter,Δwidth,ΔheightΔ<em>x<strong>c</strong>e<strong>n</strong>t<strong>e</strong>r</em>,Δ<em>y<strong>c</strong>e<strong>n</strong>t<strong>e</strong>r</em>,Δ<em>w<strong>i</strong>d<strong>t</strong>h</em>,Δ<em>h<strong>e</strong>i<strong>g</strong>h**t</em>, which are usually small values that tweak the reference box to better fit what we want.</p>
<p><strong>Anchors</strong> are fixed bounding boxes that are placed throughout the image with different sizes and ratios that are going to be used for reference when first predicting object locations.</p>
<p>Since we are working with a convolutional feature map of size convwidth×convheight×convdepth<em>c<strong>o</strong>n<strong>v</strong>w<strong>i</strong>d<strong>t</strong>h</em>×<em>c<strong>o</strong>n<strong>v</strong>h<strong>e</strong>i<strong>g</strong>h**t</em>×<em>c<strong>o</strong>n<strong>v</strong>d<strong>e</strong>p<strong>t</strong>h</em>, we create a set of anchors for each of the points in convwidth×convheight<em>c<strong>o</strong>n<strong>v</strong>w<strong>i</strong>d<strong>t</strong>h</em>×<em>c<strong>o</strong>n<strong>v</strong>h<strong>e</strong>i<strong>g</strong>h**t</em>. It’s important to understand that even though anchors are defined based on the convolutional feature map, the final anchors reference the original image.</p>
<p>Since we only have convolutional and pooling layers, the dimensions of the feature map will be proportional to those of the original image. Mathematically, if the image was w×h<em>w</em>×<em>h</em>, the feature map will end up w/r×h/r<em>w</em>/<em>r</em>×<em>h</em>/<em>r</em> where r<em>r</em> is called <em>subsampling ratio</em>. If we define one anchor per spatial position of the feature map, the final image will end up with a bunch of anchors separated by r<em>r</em> pixels. In the case of VGG, r=16<em>r</em>=16.</p>
<p><img src="https://tryolabs.com/images/blog/post-images/2018-01-18-faster-rcnn/anchors-centers.141181d6.png" alt="img">Anchor centers throught the original image</p>
<p>In order to choose the set of anchors we usually define a set of sizes (e.g. 64px, 128px, 256px) and a set of ratios between width and height of boxes (e.g. 0.5, 1, 1.5) and use all the possible combinations of sizes and ratios.</p>
<p><img src="https://tryolabs.com/images/blog/post-images/2018-01-18-faster-rcnn/anchors-progress.119e1e92.png" alt="img">Left: Anchors, Center: Anchor for a single point, Right: All anchors</p>
<h1 id="Region-Proposal-Network"><a href="#Region-Proposal-Network" class="headerlink" title="Region Proposal Network"></a>Region Proposal Network</h1><p><img src="https://tryolabs.com/images/blog/post-images/2018-01-18-faster-rcnn/rpn-architecture.99b6c089.png" alt="img">The RPN takes the convolutional feature map and generates proposals over the image</p>
<p>As we mentioned before, the RPN takes all the reference boxes (anchors) and outputs a set of good proposals for objects. It does this by having two different outputs for each of the anchors.</p>
<p>The first one is the probability that an anchor is an object. An “objectness score”, if you will. Note that the RPN doesn’t care what <em>class</em> of object it is, only that it does in fact look like an object (and not background). We are going to use this objectness score to filter out the bad predictions for the second stage. The second output is the bounding box regression for adjusting the anchors to better fit the object it’s predicting.</p>
<p>The RPN is implemented efficiently in a fully convolutional way, using the convolutional feature map returned by the base network as an input. First, we use a convolutional layer with 512 channels and 3x3 kernel size and then we have two parallel convolutional layers using a 1x11<em>x</em>1 kernel, whose number of channels depends on the number of anchors per point.</p>
<p><img src="https://tryolabs.com/images/blog/post-images/2018-01-18-faster-rcnn/rpn-conv-layers.63c5bf86.png" alt="img">Convolutional implementation of an RPN architecture, where k is the number of anchors.</p>
<p>For the classification layer, we output two predictions per anchor: the score of it being background (not an object) and the score of it being foreground (an actual object).</p>
<p>For the regression, or bounding box adjustment layer, we output 4 predictions: the deltas $Δx_{center},Δy_{center},Δwidth,Δheight$ which we will apply to the anchors to get the final proposals.</p>
<p>Using the final proposal coordinates and their “objectness” score we then have a good set of proposals for objects.</p>
<h2 id="Training-target-and-loss-functions"><a href="#Training-target-and-loss-functions" class="headerlink" title="Training, target and loss functions"></a>Training, target and loss functions</h2><p>The RPN does two different type of predictions: ==the binary classification and the bounding box regression adjustment.==</p>
<p>For training, we take all the anchors and put them into two different categories. Those that overlap a ground-truth object with an <a href="https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/" target="_blank" rel="noopener">Intersection over Union</a> (IoU) bigger than 0.5 are considered “foreground” and those that don’t overlap any ground truth object or have less than 0.1 IoU with ground-truth objects are considered “background”.</p>
<p>Then, we randomly sample those anchors to form a mini batch of size 256 — trying to maintain a balanced ratio between foreground and background anchors.</p>
<p>The RPN uses all the anchors selected for the mini batch to calculate the classification loss using <a href="https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss" target="_blank" rel="noopener">binary cross entropy</a>. Then, it uses only those minibatch anchors marked as foreground to calculate the regression loss. For calculating the targets for the regression, we use the foreground anchor and the closest ground truth object and calculate the correct ΔΔ needed to transform the anchor into the object.</p>
<p>Instead of using a simple L1 or L2 loss for the regression error, the paper suggests using Smooth L1 loss. Smooth L1 is basically L1, but when the L1 error is small enough, defined by a certain σ<em>σ</em>, the error is considered almost correct and the loss diminishes at a faster rate.</p>
<p>Using dynamic batches can be challenging for a number of reasons. Even though we try to maintain a balanced ratio between anchors that are considered background and those that are considered foreground, that is not always possible. Depending on the ground truth objects in the image and the size and ratios of the anchors, it is possible to end up with zero foreground anchors. In those cases, we turn to using the anchors with the biggest IoU to the ground truth boxes. This is far from ideal, but practical in the sense that we always have foreground samples and targets to learn from.</p>
<h2 id="Post-processing"><a href="#Post-processing" class="headerlink" title="Post processing"></a>Post processing</h2><p><strong>Non-maximum suppression</strong> Since anchors usually overlap, proposals end up also overlapping over the same object. To solve the issue of duplicate proposals we use a simple algorithmic approach called Non-Maximum Suppression (NMS). NMS takes the list of proposals sorted by score and iterateqs over the sorted list, discarding those proposals that have an IoU larger than some predefined threshold with a proposal that has a higher score.</p>
<p>While this looks simple, it is very important to be cautious with the IoU threshold. Too low and you may end up missing proposals for objects; too high and you could end up with too many proposals for the same object. A value commonly used is 0.6.</p>
<p><strong>Proposal selection</strong> After applying NMS, we keep the top N proposals sorted by score. In the paper N=2000<em>N</em>=2000 is used, but it is possible to lower that number to as little as 50 and still get quite good results.</p>
<h2 id="Standalone-application"><a href="#Standalone-application" class="headerlink" title="Standalone application"></a>Standalone application</h2><p>The RPN can be used by itself without needing the second stage model. In problems where there is only a single class of objects, the objectness probability can be used as the final class probability. This is because for this case, “foreground” = “single class” and “background” = “<strong>not</strong> single class”.</p>
<p>Some examples of machine learning problems that can benefit from a standalone usage of the RPN are the popular (but still challenging) face detection and text detection.</p>
<p>One of the advantages of using only the RPN is the gain in speed both in training and prediction. Since the RPN is a very simple network which only uses convolutional layers, the prediction time can be faster than using the classification base network.</p>
<h1 id="Region-of-Interest-Pooling"><a href="#Region-of-Interest-Pooling" class="headerlink" title="Region of Interest Pooling"></a>Region of Interest Pooling</h1><p>After the RPN step, we have a bunch of object proposals with no class assigned to them. Our next problem to solve is how to take these bounding boxes and classify them into our desired categories.</p>
<p>The simplest approach would be to take each proposal, crop it, and then pass it through the pre-trained base network. Then, we can use the extracted features as input for a vanilla image classifier. The main problem is that running the computations for all the 2000 proposals is really inefficient and slow.</p>
<p>Faster R-CNN tries to solve, or at least mitigate, this problem by reusing the existing convolutional feature map. This is done by extracting fixed-sized feature maps for each proposal using region of interest pooling. Fixed size feature maps are needed for the R-CNN in order to classify them into a fixed number of classes.</p>
<p><img src="https://tryolabs.com/images/blog/post-images/2018-01-18-faster-rcnn/roi-architecture.7eaae6c2.png" alt="img">Region of Interest Pooling</p>
<p>A simpler method, which is widely used by object detection implementations, including Luminoth’s Faster R-CNN, is to crop the convolutional feature map using each proposal and then resize each crop to a fixed sized 14×14×convdepth14×14×<em>c<strong>o</strong>n<strong>v</strong>d<strong>e</strong>p<strong>t</strong>h</em>using interpolation (usually bilinear). After cropping, max pooling with a 2x2 kernel is used to get a final 7×7×convdepth7×7×<em>c<strong>o</strong>n<strong>v</strong>d<strong>e</strong>p<strong>t</strong>h</em> feature map for each proposal.</p>
<p>The reason for choosing those exact shapes is related to how it is used next by the next block (R-CNN). It is important to understand that those are customizable depending on the second stage use.</p>
<h1 id="Region-based-Convolutional-Neural-Network"><a href="#Region-based-Convolutional-Neural-Network" class="headerlink" title="Region-based Convolutional Neural Network"></a>Region-based Convolutional Neural Network</h1><p>Region-based convolutional neural network (R-CNN) is the final step in Faster R-CNN’s pipeline. After getting a convolutional feature map from the image, using it to get object proposals with the RPN and finally extracting features for each of those proposals (via RoI Pooling), we finally need to use these features for classification. R-CNN tries to mimic the final stages of classification CNNs where a fully-connected layer is used to output a score for each possible object class.</p>
<p>R-CNN has two different goals:</p>
<ol>
<li>Classify proposals into one of the classes, plus a background class (for removing bad proposals).</li>
<li>Better adjust the bounding box for the proposal according to the predicted class.</li>
</ol>
<p>In the original Faster R-CNN paper, the R-CNN takes the feature map for each proposal, flattens it and uses two fully-connected layers of size 4096 with ReLU activation.</p>
<p>Then, it uses two different fully-connected layers for each of the different objects:</p>
<ul>
<li>A fully-connected layer with N+1<em>N</em>+1 units where N<em>N</em> is the total number of classes and that extra one is for the background class.</li>
<li>A fully-connected layer with 4N4<em>N</em> units. We want to have a regression prediction, thus we need Δcenterx,Δcentery,Δwidth,ΔheightΔ<em>c<strong>e</strong>n<strong>t</strong>e<strong>r</strong>x</em>,Δ<em>c<strong>e</strong>n<strong>t</strong>e<strong>r</strong>y</em>,Δ<em>w<strong>i</strong>d<strong>t</strong>h</em>,Δ<em>h<strong>e</strong>i<strong>g</strong>h**t</em> for each of the N possible classes.</li>
</ul>
<p><img src="https://tryolabs.com/images/blog/post-images/2018-01-18-faster-rcnn/rcnn-architecture.6732b9bd.png" alt="img">R-CNN architecture</p>
<h2 id="Training-and-targets"><a href="#Training-and-targets" class="headerlink" title="Training and targets"></a>Training and targets</h2><p>Targets for R-CNN are calculated in almost the same way as the RPN targets, but taking into account the different possible classes. We take the proposals and the ground-truth boxes, and calculate the IoU between them.</p>
<p>Those proposals that have a IoU greater than 0.5 with any ground truth box get assigned to that ground truth. Those that have between 0.1 and 0.5 get labeled as background. Contrary to what we did while assembling targets for the RPN, we ignore proposals without any intersection. This is because at this stage we are assuming that we have good proposals and we are more interested in solving the harder cases. Of course, all these values are hyperparameters that can be tuned to better fit the type of objects that you are trying to find.</p>
<p>The targets for the bounding box regression are calculated as the offset between the proposal and its corresponding ground-truth box, only for those proposals that have been assigned a class based on the IoU threshold.</p>
<p>We randomly sample a balanced mini batch of size 64 in which we have up to 25% foreground proposals (with class) and 75% background.</p>
<p>Following the same path as we did for the RPNs losses, the classification loss is now a multiclass cross entropy loss, using all the selected proposals and the Smooth L1 loss for the 25% proposals that are matched to a ground truth box. We have to be careful when getting that loss since the output of the R-CNN fully connected network for bounding box regressions has one prediction for each of the classes. When calculating the loss, we only have to take into account the one for the correct class.</p>
<h2 id="Post-processing-1"><a href="#Post-processing-1" class="headerlink" title="Post processing"></a>Post processing</h2><p>Similar to the RPN, we end up with a bunch of objects with classes assigned which need further processing before returning them.</p>
<p>In order to apply the bounding box adjustments we have to take into account which is the class with the highest probability for that proposal. We also have to ignore those proposals that have the background class as the one with the highest probability.</p>
<p>After getting the final objects and ignoring those predicted as background, we apply class-based NMS. This is done by grouping the objects by class, sorting them by probability and then applying NMS to each independent group before joining them again.</p>
<p>For our final list of objects, we also can set a probability threshold and a limit on the number of objects for each class.</p>
<h1 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h1><p>In the original paper, Faster R-CNN was trained using a multi-step approach, training parts independently and merging the trained weights before a final full training approach. Since then, it has been found that doing end-to-end, joint training leads to better results.</p>
<p>After putting the complete model together we end up with 4 different losses, two for the RPN and two for R-CNN. We have the trainable layers in RPN and R-CNN, and we also have the base network which we can train (fine-tune) or not.</p>
<p>The decision to train the base network depends on the nature of the objects we want to learn and the computing power available. If we want to detect objects that are similar to those that were on the original dataset on which the base network was trained on, then there is no real need except for trying to squeeze all the possible performance we can get. On the other hand, training the base network can be expensive both in time and on the necessary hardware, to be able to fit the complete gradients.</p>
<p>The four different losses are combined using a weighted sum. This is because we may want to give classification losses more weight relative to regression ones, or maybe give R-CNN losses more power over the RPNs’.</p>
<p>Apart from the regular losses, we also have the regularization losses which we skipped for the sake of brevity but can be defined both in RPN and in R-CNN. We use L2 regularization for some of the layers and depending on which base network being used and if it’s trained, it may also have regularization.</p>
<p>We train using Stochastic Gradient Descent with momentum, setting the momentum value to 0.9. You can easily train Faster R-CNN with any other optimizer without bumping into any big problem.</p>
<p>The learning rate starts at 0.0010.001 and then decreases to 0.00010.0001 after 50K steps. This is one of the hyperparameters that usually matters the most. When training with Luminoth, we usually start with the defaults and tune it from then on.</p>
<h1 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h1><p>The evaluation is done using the standard Mean Average Precision (mAP) at some specific IoU threshold (e.g. <a href="mailto:mAP@0.5" target="_blank" rel="noopener">mAP@0.5</a>). mAP is a metric that comes from information retrieval, and is commonly used for calculating the error in ranking problems and for evaluating object detection problems.</p>
<p>We won’t go into details since these type of metrics deserve a blogpost of their own, but the important takeway is that mAP penalizes you when you miss a box that you should have detected, as well as when you detect something that does not exist or detect the same thing multiple times.</p>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>By now, you should have a clear idea of how Faster R-CNN works, why some decisions have been made and some idea on how to be able to tweak it for your specific case. If you want to get a deeper understanding on how it works you should check <a href="https://github.com/tryolabs/luminoth/tree/master/luminoth/models/fasterrcnn" target="_blank" rel="noopener">Luminoth’s implementation</a>.</p>
<p>Faster R-CNN is one of the models that proved that it is possible to solve complex computer vision problems with the same principles that showed such amazing results at the start of this new deep learning revolution.</p>
<p>New models are currently being built, not only for object detection, but for semantic segmentation, 3D-object detection, and more, that are based on this original model. Some borrow the RPN, some borrow the R-CNN, others just build on top of both. This is why it is important to fully understand what is under the hood so we are better prepared to tackle future problems.</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference:"></a>Reference:</h2><p><a href="https://tryolabs.com/blog/2018/01/18/faster-r-cnn-down-the-rabbit-hole-of-modern-object-detection/" target="_blank" rel="noopener">Tryolabs Faster R-CNN</a></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/04/14/SLAM/basicConcepts/" rel="next" title>
                <i class="fa fa-chevron-left"></i> 
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/04/14/SLAM/CNN/" rel="prev" title>
                 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/profile.JPG" alt="May">
            
              <p class="site-author-name" itemprop="name">May</p>
              <p class="site-description motion-element" itemprop="description">Life is fantastic!</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">163</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">标签</span>
                
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/sukamay" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Faster-R-CNN-Down-the-rabbit-hole-of-modern-object-detection"><span class="nav-text">Faster R-CNN: Down the rabbit hole of modern object detection</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Background"><span class="nav-text">Background</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Architecture"><span class="nav-text">Architecture</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Base-network"><span class="nav-text">Base network</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#VGG"><span class="nav-text">VGG</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#VGG-vs-ResNet"><span class="nav-text">VGG vs ResNet</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Anchors"><span class="nav-text">Anchors</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Region-Proposal-Network"><span class="nav-text">Region Proposal Network</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Training-target-and-loss-functions"><span class="nav-text">Training, target and loss functions</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Post-processing"><span class="nav-text">Post processing</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Standalone-application"><span class="nav-text">Standalone application</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Region-of-Interest-Pooling"><span class="nav-text">Region of Interest Pooling</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Region-based-Convolutional-Neural-Network"><span class="nav-text">Region-based Convolutional Neural Network</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Training-and-targets"><span class="nav-text">Training and targets</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Post-processing-1"><span class="nav-text">Post processing</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Training"><span class="nav-text">Training</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Evaluation"><span class="nav-text">Evaluation</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Conclusion"><span class="nav-text">Conclusion</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Reference"><span class="nav-text">Reference:</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">May</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>


<div>
<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<span id="busuanzi_container_site_pv" style="display:none">
    本站总访问量 <span id="busuanzi_value_site_pv"></span> 次
    <span class="post-meta-divider">|</span>
</span>
<span id="busuanzi_container_site_uv" style="display:none">
    有<span id="busuanzi_value_site_uv"></span>人看过我的博客啦
</span>
</div>



        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  

    
      <script id="dsq-count-scr" src="https://sukamay.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'https://sukamay.github.io/2019/04/14/SLAM/Faster R-CNN/';
          this.page.identifier = '2019/04/14/SLAM/Faster R-CNN/';
          this.page.title = '';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://sukamay.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  

















  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  

  

  

</body>
</html>
